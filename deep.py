# -*- coding: utf-8 -*-
"""DEEP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jt-Teu9aBDM9lV6Iz0i2AVMErsBi8_1i
"""

!pip install timm

!pip install "fsspec<=2023.9.2"

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets as torchvision_datasets # ì´ë¦„ ì¶©ëŒ ë°©ì§€
from torchvision import transforms
import timm
from timm.data.auto_augment import rand_augment_transform
from timm.scheduler import CosineLRScheduler
from datasets import load_dataset # Hugging Face datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from tqdm import tqdm
import os
import time

# --- A100 ë° 2ì‹œê°„ ì œì•½ ìµœì í™” ì„¤ì • ---
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

MODEL_NAME = 'tf_efficientnetv2_m.in21k'
IMG_SIZE = 256
BATCH_SIZE = 256
EPOCHS = 80
SAVE_DIR = '/content/drive/MyDrive/CIFAR100_A100_2hr/'

DEVICE = torch.device("cuda")
NUM_CLASSES = 100
LEARNING_RATE = 1e-3
WEIGHT_DECAY = 0.05
LABEL_SMOOTHING = 0.1
WARMUP_EPOCHS = 5
CLIP_GRAD_NORM = 1.0

os.makedirs(SAVE_DIR, exist_ok=True)

CIFAR100_MEAN = [0.5071, 0.4867, 0.4408]
CIFAR100_STD = [0.2675, 0.2565, 0.2761]


# ========================== í•µì‹¬ ìˆ˜ì • ì‚¬í•­: ë°ì´í„° ë¡œë”© ë°©ì‹ ë³€ê²½ ==========================
# Hugging Face 'datasets' ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì¶œë ¥ì„ PyTorch DataLoaderê°€ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ ëž˜í¼ í´ëž˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class HuggingFace_CIFAR100(Dataset):
    def __init__(self, hf_dataset, transform=None):
        self.hf_dataset = hf_dataset
        self.transform = transform

    def __len__(self):
        return len(self.hf_dataset)

    def __getitem__(self, idx):
        # Hugging Face ë°ì´í„°ì…‹ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ {'img': PIL Image, 'fine_label': int}
        image = self.hf_dataset[idx]['img']
        label = self.hf_dataset[idx]['fine_label'] # 100ê°œ í´ëž˜ìŠ¤ëŠ” 'fine_label'ì„ ì‚¬ìš©

        if self.transform:
            image = self.transform(image)

        return image, label

def get_dataloaders(batch_size, img_size):
    train_transform = transforms.Compose([
        transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(),
        rand_augment_transform('rand-m9-mstd0.5-inc1', {}),
        transforms.ToTensor(),
        transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD),
        transforms.RandomErasing(p=0.25)
    ])
    val_transform = transforms.Compose([
        transforms.Resize(img_size),
        transforms.CenterCrop(img_size),
        transforms.ToTensor(),
        transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD),
    ])

    # torchvision ëŒ€ì‹  Hugging Faceì˜ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ CIFAR-100 ë¡œë“œ
    print("Downloading CIFAR-100 using Hugging Face's 'datasets' library...")
    hf_cifar100 = load_dataset("cifar100")
    print("Download complete.")

    # ìœ„ì—ì„œ ì •ì˜í•œ ëž˜í¼ í´ëž˜ìŠ¤ë¡œ PyTorch Dataset ê°ì²´ ìƒì„±
    train_dataset = HuggingFace_CIFAR100(hf_cifar100['train'], transform=train_transform)
    # CIFAR-100ì˜ í…ŒìŠ¤íŠ¸ì…‹ì„ validation ìš©ë„ë¡œ ì‚¬ìš©
    val_dataset = HuggingFace_CIFAR100(hf_cifar100['test'], transform=val_transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)
    return train_loader, val_loader

# ========================== ìˆ˜ì • ë. ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼ ==========================

def train_one_epoch(model, loader, optimizer, criterion, scaler, mixup_fn, lr_scheduler, epoch):
    model.train()
    pbar = tqdm(loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Training]")
    for inputs, targets in pbar:
        inputs, targets = inputs.to(DEVICE, non_blocking=True), targets.to(DEVICE, non_blocking=True)
        inputs, targets = mixup_fn(inputs, targets)
        optimizer.zero_grad(set_to_none=True)
        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
            outputs = model(inputs)
            loss = criterion(outputs, targets)
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)
        scaler.step(optimizer)
        scaler.update()
        pbar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])
    lr_scheduler.step(epoch + 1)

@torch.no_grad()
def evaluate(model, loader):
    model.eval()
    total_correct, total_samples = 0, 0
    pbar = tqdm(loader, desc="[Validation]")
    for inputs, targets in pbar:
        inputs, targets = inputs.to(DEVICE, non_blocking=True), targets.to(DEVICE, non_blocking=True)
        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
            outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total_correct += (predicted == targets).sum().item()
        total_samples += targets.size(0)
    return total_correct / total_samples

# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
start_time = time.time()
print(f"PyTorch Version: {torch.__version__}")
print(f"Using device: {DEVICE} ({torch.cuda.get_device_name(DEVICE)})")
print(f"2-Hour Optimized Settings: Model={MODEL_NAME}, Image Size={IMG_SIZE}, Batch Size={BATCH_SIZE}, Epochs={EPOCHS}")
print(f"Models will be saved to: {SAVE_DIR}")

train_loader, val_loader = get_dataloaders(BATCH_SIZE, IMG_SIZE)
model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)

if torch.__version__ >= "2.0":
    print("Applying torch.compile() for A100 optimization...")
    model = torch.compile(model)

criterion = nn.CrossEntropyLoss().to(DEVICE)
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
lr_scheduler = CosineLRScheduler(optimizer, t_initial=EPOCHS, lr_min=1e-6, warmup_t=WARMUP_EPOCHS, warmup_lr_init=1e-7, warmup_prefix=True)
scaler = torch.amp.GradScaler('cuda')
mixup_fn = timm.data.Mixup(mixup_alpha=0.8, cutmix_alpha=1.0, prob=1.0, switch_prob=0.5, mode='batch', label_smoothing=LABEL_SMOOTHING, num_classes=NUM_CLASSES)

best_val_acc = 0.0
for epoch in range(EPOCHS):
    epoch_start_time = time.time()
    train_one_epoch(model, train_loader, optimizer, criterion, scaler, mixup_fn, lr_scheduler, epoch)
    val_acc = evaluate(model, val_loader)
    epoch_end_time = time.time()

    print(f"\nEpoch {epoch+1}/{EPOCHS}: Val Accuracy: {val_acc * 100:.2f}% | Epoch Time: {epoch_end_time - epoch_start_time:.2f}s")
    print("-" * 50)
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        save_path = os.path.join(SAVE_DIR, f'{MODEL_NAME.replace(".", "_")}_cifar100_best_2hr.pth')
        try:
            torch.save(model.state_dict(), save_path)
        except Exception:
            torch.save(model._orig_mod.state_dict(), save_path)
        print(f"ðŸŽ‰ New best model saved to Google Drive with accuracy: {best_val_acc * 100:.2f}%")

end_time = time.time()
print(f"âœ¨ Training finished in {(end_time - start_time) / 60:.2f} minutes. Best validation accuracy: {best_val_acc * 100:.2f}%")