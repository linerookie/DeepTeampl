# -*- coding: utf-8 -*-
"""DEEP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jt-Teu9aBDM9lV6Iz0i2AVMErsBi8_1i
"""

!pip install timm

!pip install "fsspec<=2023.9.2"

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets as torchvision_datasets # 이름 충돌 방지
from torchvision import transforms
import timm
from timm.data.auto_augment import rand_augment_transform
from timm.scheduler import CosineLRScheduler
from datasets import load_dataset # Hugging Face datasets 라이브러리 import
from tqdm import tqdm
import os
import time

# --- A100 및 2시간 제약 최적화 설정 ---
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

MODEL_NAME = 'tf_efficientnetv2_m.in21k'
IMG_SIZE = 256
BATCH_SIZE = 256
EPOCHS = 80
SAVE_DIR = '/content/drive/MyDrive/CIFAR100_A100_2hr/'

DEVICE = torch.device("cuda")
NUM_CLASSES = 100
LEARNING_RATE = 1e-3
WEIGHT_DECAY = 0.05
LABEL_SMOOTHING = 0.1
WARMUP_EPOCHS = 5
CLIP_GRAD_NORM = 1.0

os.makedirs(SAVE_DIR, exist_ok=True)

CIFAR100_MEAN = [0.5071, 0.4867, 0.4408]
CIFAR100_STD = [0.2675, 0.2565, 0.2761]


# ========================== 핵심 수정 사항: 데이터 로딩 방식 변경 ==========================
# Hugging Face 'datasets' 라이브러리의 출력을 PyTorch DataLoader가 사용할 수 있도록 래퍼 클래스를 정의합니다.
class HuggingFace_CIFAR100(Dataset):
    def __init__(self, hf_dataset, transform=None):
        self.hf_dataset = hf_dataset
        self.transform = transform

    def __len__(self):
        return len(self.hf_dataset)

    def __getitem__(self, idx):
        # Hugging Face 데이터셋은 딕셔너리 형태 {'img': PIL Image, 'fine_label': int}
        image = self.hf_dataset[idx]['img']
        label = self.hf_dataset[idx]['fine_label'] # 100개 클래스는 'fine_label'을 사용

        if self.transform:
            image = self.transform(image)

        return image, label

def get_dataloaders(batch_size, img_size):
    train_transform = transforms.Compose([
        transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(),
        rand_augment_transform('rand-m9-mstd0.5-inc1', {}),
        transforms.ToTensor(),
        transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD),
        transforms.RandomErasing(p=0.25)
    ])
    val_transform = transforms.Compose([
        transforms.Resize(img_size),
        transforms.CenterCrop(img_size),
        transforms.ToTensor(),
        transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD),
    ])

    # torchvision 대신 Hugging Face의 datasets 라이브러리로 CIFAR-100 로드
    print("Downloading CIFAR-100 using Hugging Face's 'datasets' library...")
    hf_cifar100 = load_dataset("cifar100")
    print("Download complete.")

    # 위에서 정의한 래퍼 클래스로 PyTorch Dataset 객체 생성
    train_dataset = HuggingFace_CIFAR100(hf_cifar100['train'], transform=train_transform)
    # CIFAR-100의 테스트셋을 validation 용도로 사용
    val_dataset = HuggingFace_CIFAR100(hf_cifar100['test'], transform=val_transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)
    return train_loader, val_loader

# ========================== 수정 끝. 나머지 코드는 이전과 동일 ==========================

def train_one_epoch(model, loader, optimizer, criterion, scaler, mixup_fn, lr_scheduler, epoch):
    model.train()
    pbar = tqdm(loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Training]")
    for inputs, targets in pbar:
        inputs, targets = inputs.to(DEVICE, non_blocking=True), targets.to(DEVICE, non_blocking=True)
        inputs, targets = mixup_fn(inputs, targets)
        optimizer.zero_grad(set_to_none=True)
        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
            outputs = model(inputs)
            loss = criterion(outputs, targets)
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)
        scaler.step(optimizer)
        scaler.update()
        pbar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])
    lr_scheduler.step(epoch + 1)

@torch.no_grad()
def evaluate(model, loader):
    model.eval()
    total_correct, total_samples = 0, 0
    pbar = tqdm(loader, desc="[Validation]")
    for inputs, targets in pbar:
        inputs, targets = inputs.to(DEVICE, non_blocking=True), targets.to(DEVICE, non_blocking=True)
        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
            outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total_correct += (predicted == targets).sum().item()
        total_samples += targets.size(0)
    return total_correct / total_samples

# --- 메인 실행 로직 ---
start_time = time.time()
print(f"PyTorch Version: {torch.__version__}")
print(f"Using device: {DEVICE} ({torch.cuda.get_device_name(DEVICE)})")
print(f"2-Hour Optimized Settings: Model={MODEL_NAME}, Image Size={IMG_SIZE}, Batch Size={BATCH_SIZE}, Epochs={EPOCHS}")
print(f"Models will be saved to: {SAVE_DIR}")

train_loader, val_loader = get_dataloaders(BATCH_SIZE, IMG_SIZE)
model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)

if torch.__version__ >= "2.0":
    print("Applying torch.compile() for A100 optimization...")
    model = torch.compile(model)

criterion = nn.CrossEntropyLoss().to(DEVICE)
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
lr_scheduler = CosineLRScheduler(optimizer, t_initial=EPOCHS, lr_min=1e-6, warmup_t=WARMUP_EPOCHS, warmup_lr_init=1e-7, warmup_prefix=True)
scaler = torch.amp.GradScaler('cuda')
mixup_fn = timm.data.Mixup(mixup_alpha=0.8, cutmix_alpha=1.0, prob=1.0, switch_prob=0.5, mode='batch', label_smoothing=LABEL_SMOOTHING, num_classes=NUM_CLASSES)

best_val_acc = 0.0
for epoch in range(EPOCHS):
    epoch_start_time = time.time()
    train_one_epoch(model, train_loader, optimizer, criterion, scaler, mixup_fn, lr_scheduler, epoch)
    val_acc = evaluate(model, val_loader)
    epoch_end_time = time.time()

    print(f"\nEpoch {epoch+1}/{EPOCHS}: Val Accuracy: {val_acc * 100:.2f}% | Epoch Time: {epoch_end_time - epoch_start_time:.2f}s")
    print("-" * 50)
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        save_path = os.path.join(SAVE_DIR, f'{MODEL_NAME.replace(".", "_")}_cifar100_best_2hr.pth')
        try:
            torch.save(model.state_dict(), save_path)
        except Exception:
            torch.save(model._orig_mod.state_dict(), save_path)
        print(f"🎉 New best model saved to Google Drive with accuracy: {best_val_acc * 100:.2f}%")

end_time = time.time()
print(f"✨ Training finished in {(end_time - start_time) / 60:.2f} minutes. Best validation accuracy: {best_val_acc * 100:.2f}%")